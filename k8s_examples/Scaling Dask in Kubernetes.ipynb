{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling Dask in Kubernetes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "1. Dask overview\n",
    "2. Kubernetes overview\n",
    "3. Overview of installing Dask\n",
    "    1. Installation using Helm\n",
    "    2. Other installation methods\n",
    "4. Overview of integrating Dask and Kubernetes\n",
    "    1. Scaling up/down cluster using dask_kubernetes\n",
    "5. Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask Overview\n",
    "\n",
    "Dask is a flexible library for parallel computing in Python. It is purpose-built to parallelize python data science applications from a single laptop all the way up  to a complex 100+ node cluster.\n",
    "\n",
    "It is composed of a Dask scheduler and a scaling number of Dask workers and the APIs are designed to be familiar for anyone who has used Pandas or Numpy in the past.\n",
    "\n",
    "By itself Dask accelerates many machine learning applications, but when paired with the additional acclerations of GPUs integrated through the RAPIDS modules it becomes a very powerful tool that no data scientist should be without. \n",
    "\n",
    "For more information about dask see [here](https://docs.dask.org/en/latest/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kubernetes\n",
    "Kubernetes (or K8S) is an open source tool for managing container workloads and services. K8S is designed to scale, and can run on single node systems all the way up to entire clouds.\n",
    "\n",
    "K8S allows you to deploy docker containers to run tasks. These docker containers are deployed in pods, which can have resources limitations defined, execution commands set, and allows you to specify custom docker images.\n",
    "\n",
    "K8S allows dynamic resources addition/removal and can be run on-prem, in the cloud, or using hybrid models.\n",
    "\n",
    "For more information about Kubernetes see [here](https://kubernetes.io/docs/home/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask and Kubernetes Integration\n",
    "\n",
    "By combining the cluster-management and auto-scaling capabilities of Kubernetes with the parallel computing and distributed resource management capabilities of dask we can create a data science environment that dynamically determines resources needs, grows to meets those needs, and optimally executes all data processesing, training, and inferencing in our cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of installing Dask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation using Helm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XXX: These must  be run on the K8S managment server, not through this pod.\n",
    "# Additional steps can be found in the DeepOps project here: https://github.com/NVIDIA/deepops/blob/master/docs/rapids-dask.md\n",
    "!helm install -n rapids --namespace rapids --values helm/rapids.yml stable/dask\n",
    "!kubectl create -f k8s/roles.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask can also be installed by using a series of K8S YAML files or by manually deploying and configuring pods. There is more information regarding these methods available [here](http://kubernetes.dask.org/en/latest/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of integrating Dask and Kubernetes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling up/down cluster using dask_kubernetes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manual\n",
    "First we will need to create a client that will connect to the existing Dask scheduler.\n",
    "\n",
    "We can see the current resources available, the number of workers, and links to the scheduler and dashboard. Note that the links may not be valid depending on your environment and you may need to use the IPs and ports designated by the cluster admin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask_kubernetes as dk\n",
    "from dask.distributed import Client\n",
    "\n",
    "client = Client()\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will define a specification for our workers. It is best practice to use the same Docker image for the workers as this notebook. It is also necessary to set the dask-worker resources args to match the K8S resources. Otherwise K8S might kill run Dask jobs.\n",
    "\n",
    "We pull the scheduler URL out of the client object and write it to our yaml file so that the worker knows who to communicate with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worker_spec_fname = '/worker_spec.yaml'\n",
    "worker_spec = '''\n",
    "# worker-spec.yml\n",
    "\n",
    "kind: Pod\n",
    "metadata:\n",
    "  labels:\n",
    "    foo: bar\n",
    "spec:\n",
    "  restartPolicy: Never\n",
    "  containers:\n",
    "  - image: supertetelman/k8s-rapids-dask:cuda9.2-runtime-ubuntu16.04\n",
    "    imagePullPolicy: IfNotPresent\n",
    "    args: [dask-worker, {}, --nthreads, '1', --no-bokeh, --memory-limit, 6GB, --no-bokeh, --death-timeout, '60']\n",
    "    name: dask\n",
    "    resources:\n",
    "      limits:\n",
    "        cpu: \"2\"\n",
    "        memory: 6G\n",
    "        nvidia.com/gpu: 0\n",
    "      requests:\n",
    "        cpu: \"2\"\n",
    "        memory: 6G\n",
    "        nvidia.com/gpu: 0\n",
    "'''.format(client._start_arg) # Note that we are telling the worker to communicate with the scheduler that has already been launched\n",
    "\n",
    "with open(worker_spec_fname, \"w\") as yaml_file:\n",
    "    yaml_file.write(worker_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define the Dask Kubernetes worker cluster. After this all Dask workers launched will match the spec we just defined. As a best practice, all workers in the cluster should be identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = dk.KubeCluster.from_yaml(worker_spec_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should now see that the cluster has not been modified yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now manually scale up the cluster to contain N workers.\n",
    "\n",
    "This can take some time to complete. If the `client` command output does not change try waiting 30 seconds and running it again.\n",
    "\n",
    "Be sure not to launch more workers than you have available resources in your cluster. Doing this may require manually deleting pods through the Kubernetes interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can scale up some more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we can scale down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that we know the basics of installing Dask into K8S, defining worker nodes, and scaling a Dask cluster we can build a machine learning pipeline to take advantage of the parallelism. \n",
    "\n",
    "In the next notebook we'll take a look at creating a single worker Dask workload and then see how easy it is to accelerate via scaling in Kubernetes.\n",
    "\n",
    "After using parallesim to scale across nodes we will integrate the RAPIDS libary and accelerate the application further using GPU enabled machine learning libraries.\n",
    "\n",
    "Later, we'll also touch upon best-practices for sharing large volumes and storage across your Kubernetes cluster for Dask to use."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
