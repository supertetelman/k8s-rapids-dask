{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling Dask in Kubernetes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "1. Dask overview\n",
    "2. Kubernetes overview\n",
    "3. Overview of installing Dask\n",
    "    1. Installation using Helm\n",
    "    2. Other installation methods\n",
    "4. Overview of integrating Dask and Kubernetes\n",
    "    1. Scaling up/down cluster using dask_kubernetes\n",
    "5. Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask Overview\n",
    "\n",
    "Dask is a flexible library for parallel computing in Python. It is purpose-built to parallelize python data science applications from a single laptop all the way up  to a complex 100+ node cluster.\n",
    "\n",
    "It is composed of a Dask scheduler and a scaling number of Dask workers and the APIs are designed to be familiar for anyone who has used Pandas or Numpy in the past.\n",
    "\n",
    "By itself Dask accelerates many machine learning applications, but when paired with the additional acclerations of GPUs integrated through the RAPIDS modules it becomes a very powerful tool that no data scientist should be without. \n",
    "\n",
    "For more information about dask see [here](https://docs.dask.org/en/latest/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kubernetes\n",
    "Kubernetes (or K8S) is an open source tool for managing container workloads and services. K8S is designed to scale and can run on single node systems all the way up to entire clouds.\n",
    "\n",
    "K8S allows you to deploy docker containers to run tasks. These docker containers are deployed in pods, which can have resources limitations defined, execution commands set, and allows you to specify custom docker images.\n",
    "\n",
    "K8S allows dynamic resources addition/removal and can be run on-prem, in the cloud, or using hybrid models.\n",
    "\n",
    "For more information about Kubernetes see [here](https://kubernetes.io/docs/home/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask and Kubernetes Integration\n",
    "\n",
    "By combining the cluster-management and auto-scaling capabilities of Kubernetes with the parallel computing and distributed resource management capabilities of dask we can create a data science environment that dynamically determines resources needs, grows to meets those needs, and optimally executes all data processesing, training, and inferencing in our cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask Installation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Installation using Helm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# XXX: These must  be run on the K8S managment server, not through this pod.\n",
    "# Additional steps can be found in the DeepOps project here: https://github.com/NVIDIA/deepops/blob/master/docs/rapids-dask.md\n",
    "\n",
    "helm install -n rapids --namespace rapids --values helm/rapids.yml stable/dask\n",
    "kubectl create -f k8s/roles.yaml\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manual Installation\n",
    "\n",
    "Dask can also be installed by using a series of K8S YAML files or by manually deploying and configuring pods. There is more information regarding these methods available [here](http://kubernetes.dask.org/en/latest/). This requires a Docker images with the `dask_kubernetes` library installed and a K8S service account with enough permissions to start and stop pods.\n",
    "\n",
    "\n",
    "#### Installation using DeepOps\n",
    "\n",
    "The [DeepOps](https://github.com/NVIDIA/deepops/blob/master/) project offers two different deployment methods for Dask. There is a [standalone deployment script](https://github.com/NVIDIA/deepops/blob/master/docs/rapids-dask.md) which wraps the helm install and there is a Kubeflow deployment script which will deploy Kubeflow along with the required Dask service accounts. The `supertetelman/k8s-rapids-dask:0.9-cuda10.0-runtime-ubuntu18.04` [Docker image](https://github.com/supertetelman/k8s-rapids-dask/blob/master/Dockerfile) contains all the Dask libaries and Jupyter plugins required to manage GPU resources and your Dask cluster through the Jupyter interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Deployment\n",
    "\n",
    "#### Jupyter Lab Extension\n",
    "Dask Clusters can be deployed and managed using the Jupyter interface via the Dask extension. See the [Dask Jupyter Lab Extension GitHub](https://github.com/dask/dask-labextension) for additional usage and configuration information.\n",
    "\n",
    "#### Helm\n",
    "If you installed Dask using helm a scheduler and initial cluster was already deployed.\n",
    "\n",
    "#### dask_kubernetes\n",
    "Dask clusters can be created and managed using the Python `dask_kubernetes` libarrary.\n",
    "\n",
    "First it is necessary to define a specification for our workers. This is typically done through a yaml file. It is best practice to use the same Docker image for the workers as this notebook and to allocate a single GPU for each Dask worker. It is also necessary to set the dask-worker resources args to match the K8S resources. Otherwise K8S might kill running Dask jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask_kubernetes as dk\n",
    "import time\n",
    "from dask.distributed import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "worker_spec_fname = '/worker_spec.yaml'\n",
    "worker_spec = '''\n",
    "# worker-spec.yml\n",
    "\n",
    "kind: Pod\n",
    "metadata:\n",
    "  labels:\n",
    "    foo : bar\n",
    "spec:\n",
    "  restartPolicy: Never\n",
    "  containers:\n",
    "  - image: supertetelman/k8s-rapids-dask:0.9-cuda10.0-runtime-ubuntu18.04\n",
    "    imagePullPolicy: IfNotPresent\n",
    "    args: [dask-worker, --nthreads, '1', --no-bokeh, --memory-limit, 6GB, --no-bokeh, --death-timeout, '60']\n",
    "    name: dask\n",
    "    resources:\n",
    "      limits:\n",
    "        cpu: \"4\"\n",
    "        memory: 6G\n",
    "        nvidia.com/gpu: 1\n",
    "      requests:\n",
    "        cpu: \"4\"\n",
    "        memory: 6G\n",
    "        nvidia.com/gpu: 1\n",
    "'''\n",
    "\n",
    "with open(worker_spec_fname, \"w\") as yaml_file:\n",
    "    yaml_file.write(worker_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = dk.KubeCluster.from_yaml(worker_spec_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting to a cluster\n",
    "\n",
    "We can create a client that is connectected to the cluster we just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://10.233.69.185:41311</li>\n",
       "  <li><b>Dashboard: </b><a href='http://10.233.69.185:38445/status' target='_blank'>http://10.233.69.185:38445/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>0</li>\n",
       "  <li><b>Cores: </b>0</li>\n",
       "  <li><b>Memory: </b>0 B</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: scheduler='tcp://10.233.69.185:41311' processes=0 cores=0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling a cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manual Scaling\n",
    "We can now manually scale up the cluster to contain N workers.\n",
    "\n",
    "This can take some time to complete. The status can be dynamically monitored by executing `cluster` and watching the output. You can also manually check the status by executing `client`.\n",
    "\n",
    "Be sure not to launch more workers than you have available resources in your cluster. Also be sure not to scale up/down the cluster while pods are still in a pending state. Doing either of these actions may require manually deleting pods through the Kubernetes interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "997ece106a1c42dcb6e87280c3c8a2c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h2>KubeCluster</h2>'), HBox(children=(HTML(value='\\n<div>\\n  <style scoped>\\n    .â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://10.233.69.185:41311</li>\n",
       "  <li><b>Dashboard: </b><a href='http://10.233.69.185:38445/status' target='_blank'>http://10.233.69.185:38445/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>0</li>\n",
       "  <li><b>Cores: </b>0</li>\n",
       "  <li><b>Memory: </b>0 B</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: scheduler='tcp://10.233.69.185:41311' processes=0 cores=0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It may take some time for the image to initially be downloaded onto you node and started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://10.233.69.185:41311</li>\n",
       "  <li><b>Dashboard: </b><a href='http://10.233.69.185:38445/status' target='_blank'>http://10.233.69.185:38445/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>1</li>\n",
       "  <li><b>Cores: </b>1</li>\n",
       "  <li><b>Memory: </b>6.00 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: scheduler='tcp://10.233.69.185:41311' processes=1 cores=1>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time.sleep(5)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can scale up some more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://10.233.69.185:41311</li>\n",
       "  <li><b>Dashboard: </b><a href='http://10.233.69.185:38445/status' target='_blank'>http://10.233.69.185:38445/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>2</li>\n",
       "  <li><b>Cores: </b>2</li>\n",
       "  <li><b>Memory: </b>12.00 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: scheduler='tcp://10.233.69.185:41311' processes=2 cores=2>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster.scale(2)\n",
    "time.sleep(5)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we can scale the cluster all the way down to free up GPU resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://10.233.69.185:41311</li>\n",
       "  <li><b>Dashboard: </b><a href='http://10.233.69.185:38445/status' target='_blank'>http://10.233.69.185:38445/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>0</li>\n",
       "  <li><b>Cores: </b>0</li>\n",
       "  <li><b>Memory: </b>0 B</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: scheduler='tcp://10.233.69.185:41311' processes=0 cores=0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster.scale(0)\n",
    "time.sleep(5)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adaptive Scaling\n",
    "\n",
    "It is also possible to set dask_kubernetes to use adapative scaling. This will cause pods to be started and stopped to meet demand.\n",
    "\n",
    "Be sure to set the minimum to 1 or you may encounter timing issues that impact your machine learning code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<dask_kubernetes.adaptive.Adaptive at 0x7fbdb1cbeda0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster.adapt(minimum=1, maximum=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we scale the cluster all the way down to free up GPU resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<dask_kubernetes.adaptive.Adaptive at 0x7fbdb1cbe6a0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster.adapt(minimum=0, maximum=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that we know the basics of installing Dask into K8S, defining worker nodes, and scaling a Dask cluster we can build a machine learning pipeline to take advantage of the parallelism. \n",
    "\n",
    "In the next notebook we'll take a look at creating a single worker Dask workload and then see how easy it is to accelerate via scaling in Kubernetes.\n",
    "\n",
    "Later, we'll also touch upon best-practices for sharing large volumes and storage across your Kubernetes cluster for Dask to use."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
